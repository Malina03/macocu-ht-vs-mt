Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
***** Running Evaluation *****
  Num examples = 2731
  Batch size = 64
/data/s4314719/.envs/py3.8.6_env/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1274: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)
  label_index = (labels >= 0).nonzero()
=> Loading train corpus...
=> Loading dev corpus...
Loading model at /data/pg-macocu/MT_vs_HT/experiments/4/models/deepl/microsoft-deberta-v3-large_lr=1e-05_bsz=32_seed=1/checkpoint-1200
Generated by command:
python classifier_trf_hf.py --root_dir /data/pg-macocu/MT_vs_HT/experiments/10 --load_model /data/pg-macocu/MT_vs_HT/experiments/4/models/deepl/microsoft-deberta-v3-large_lr=1e-05_bsz=32_seed=1/checkpoint-1200 --arch microsoft/deberta-v3-large --use_majority_classification --eval
Logging training settings
 TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_steps=200,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
greater_is_better=True,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=10,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/data/pg-macocu/MT_vs_HT/experiments/10/models/deepl/microsoft-deberta-v3-large_lr=1e-05_bsz=32_seed=1_eval/runs/Mar30_15-58-02_pg-gpu07,
logging_first_step=False,
logging_steps=200,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
no_cuda=False,
num_train_epochs=10,
output_dir=/data/pg-macocu/MT_vs_HT/experiments/10/models/deepl/microsoft-deberta-v3-large_lr=1e-05_bsz=32_seed=1_eval,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=64,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=microsoft-deberta-v3-large_lr=1e-05_bsz=32_seed=1_eval,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['mlflow', 'tensorboard'],
resume_from_checkpoint=None,
run_name=/data/pg-macocu/MT_vs_HT/experiments/10/models/deepl/microsoft-deberta-v3-large_lr=1e-05_bsz=32_seed=1_eval,
save_on_each_node=False,
save_steps=200,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=2,
seed=1,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0,
)
  0%|          | 0/43 [00:00<?, ?it/s]  5%|▍         | 2/43 [00:00<00:08,  4.81it/s]  7%|▋         | 3/43 [00:00<00:11,  3.41it/s]  9%|▉         | 4/43 [00:01<00:13,  2.96it/s] 12%|█▏        | 5/43 [00:01<00:13,  2.74it/s] 14%|█▍        | 6/43 [00:02<00:14,  2.62it/s] 16%|█▋        | 7/43 [00:02<00:14,  2.55it/s] 19%|█▊        | 8/43 [00:02<00:13,  2.51it/s] 21%|██        | 9/43 [00:03<00:13,  2.48it/s] 23%|██▎       | 10/43 [00:03<00:13,  2.46it/s] 26%|██▌       | 11/43 [00:04<00:13,  2.44it/s] 28%|██▊       | 12/43 [00:04<00:12,  2.43it/s] 30%|███       | 13/43 [00:04<00:12,  2.43it/s] 33%|███▎      | 14/43 [00:05<00:11,  2.43it/s] 35%|███▍      | 15/43 [00:05<00:11,  2.42it/s] 37%|███▋      | 16/43 [00:06<00:11,  2.42it/s] 40%|███▉      | 17/43 [00:06<00:10,  2.42it/s] 42%|████▏     | 18/43 [00:07<00:10,  2.42it/s] 44%|████▍     | 19/43 [00:07<00:09,  2.42it/s] 47%|████▋     | 20/43 [00:07<00:09,  2.42it/s] 49%|████▉     | 21/43 [00:08<00:09,  2.41it/s] 51%|█████     | 22/43 [00:08<00:08,  2.41it/s] 53%|█████▎    | 23/43 [00:09<00:08,  2.42it/s] 56%|█████▌    | 24/43 [00:09<00:07,  2.42it/s] 58%|█████▊    | 25/43 [00:09<00:07,  2.42it/s] 60%|██████    | 26/43 [00:10<00:07,  2.42it/s] 63%|██████▎   | 27/43 [00:10<00:06,  2.42it/s] 65%|██████▌   | 28/43 [00:11<00:06,  2.42it/s] 67%|██████▋   | 29/43 [00:11<00:05,  2.41it/s] 70%|██████▉   | 30/43 [00:12<00:05,  2.41it/s] 72%|███████▏  | 31/43 [00:12<00:04,  2.41it/s] 74%|███████▍  | 32/43 [00:12<00:04,  2.41it/s] 77%|███████▋  | 33/43 [00:13<00:04,  2.41it/s] 79%|███████▉  | 34/43 [00:13<00:03,  2.41it/s] 81%|████████▏ | 35/43 [00:14<00:03,  2.41it/s] 84%|████████▎ | 36/43 [00:14<00:02,  2.40it/s] 86%|████████▌ | 37/43 [00:14<00:02,  2.41it/s] 88%|████████▊ | 38/43 [00:15<00:02,  2.41it/s] 91%|█████████ | 39/43 [00:15<00:01,  2.41it/s] 93%|█████████▎| 40/43 [00:16<00:01,  2.41it/s] 95%|█████████▌| 41/43 [00:16<00:00,  2.40it/s] 98%|█████████▊| 42/43 [00:16<00:00,  2.40it/s]100%|██████████| 43/43 [00:17<00:00,  2.65it/s]100%|██████████| 43/43 [00:21<00:00,  2.03it/s]

Info:
 {'eval_loss': 0.5597682595252991, 'eval_accuracy': 0.8776978417266187, 'eval_f1': 0.8764444444444444, 'eval_precision': 0.8917814113597247, 'eval_recall': 0.8770186335403727, 'eval_runtime': 18.8683, 'eval_samples_per_second': 144.74, 'eval_steps_per_second': 2.279}
