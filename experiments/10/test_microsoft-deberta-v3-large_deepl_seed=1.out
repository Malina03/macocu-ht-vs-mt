Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
***** Running Evaluation *****
  Num examples = 290
  Batch size = 64
/data/s4314719/.envs/py3.8.6_env/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1274: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)
  label_index = (labels >= 0).nonzero()
=> Loading train corpus...
=> Loading test corpus...
Loading model at /data/pg-macocu/MT_vs_HT/experiments/10/models/deepl/microsoft-deberta-v3-large_lr=1e-05_bsz=32_epochs=30_seed=1/checkpoint-200
Generated by command:
python classifier_trf_hf.py --root_dir /data/pg-macocu/MT_vs_HT/experiments/10 --load_model /data/pg-macocu/MT_vs_HT/experiments/10/models/deepl/microsoft-deberta-v3-large_lr=1e-05_bsz=32_epochs=30_seed=1/checkpoint-200 --arch microsoft/deberta-v3-large --max_length 512 --test deepl
Logging training settings
 TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_steps=200,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
greater_is_better=True,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=10,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/data/pg-macocu/MT_vs_HT/experiments/10/models/deepl/microsoft-deberta-v3-large_lr=1e-05_bsz=32_epochs=10_seed=1_test/runs/Apr29_00-11-44_pg-gpu08,
logging_first_step=False,
logging_steps=200,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
no_cuda=False,
num_train_epochs=10,
output_dir=/data/pg-macocu/MT_vs_HT/experiments/10/models/deepl/microsoft-deberta-v3-large_lr=1e-05_bsz=32_epochs=10_seed=1_test,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=64,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=microsoft-deberta-v3-large_lr=1e-05_bsz=32_epochs=10_seed=1_test,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['mlflow', 'tensorboard'],
resume_from_checkpoint=None,
run_name=/data/pg-macocu/MT_vs_HT/experiments/10/models/deepl/microsoft-deberta-v3-large_lr=1e-05_bsz=32_epochs=10_seed=1_test,
save_on_each_node=False,
save_steps=200,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=2,
seed=1,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0,
)
  0%|          | 0/5 [00:00<?, ?it/s] 40%|████      | 2/5 [00:02<00:04,  1.45s/it] 60%|██████    | 3/5 [00:05<00:04,  2.05s/it] 80%|████████  | 4/5 [00:08<00:02,  2.37s/it]100%|██████████| 5/5 [00:10<00:00,  2.10s/it]100%|██████████| 5/5 [00:14<00:00,  2.93s/it]

Info:
 {'eval_loss': 0.553524911403656, 'eval_accuracy': 0.7793103448275862, 'eval_f1': 0.775856238829042, 'eval_precision': 0.7976582695524355, 'eval_recall': 0.7793103448275862, 'eval_runtime': 14.3152, 'eval_samples_per_second': 20.258, 'eval_steps_per_second': 0.349}
