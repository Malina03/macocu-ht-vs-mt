Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
***** Running Evaluation *****
  Num examples = 2738
  Batch size = 64
/data/s4314719/.envs/py3.8.6_env/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1274: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)
  label_index = (labels >= 0).nonzero()
=> Loading train corpus...
=> Loading dev corpus...
Loading model at /data/pg-macocu/MT_vs_HT/experiments/4/models/google/microsoft-deberta-v3-large_lr=1e-05_bsz=32_seed=1/checkpoint-2800
Generated by command:
python classifier_trf_hf.py --root_dir /data/pg-macocu/MT_vs_HT/experiments/10 --load_model /data/pg-macocu/MT_vs_HT/experiments/4/models/google/microsoft-deberta-v3-large_lr=1e-05_bsz=32_seed=1/checkpoint-2800 --arch microsoft/deberta-v3-large --use_majority_classification --eval --use_google_data
Logging training settings
 TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_steps=200,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
greater_is_better=True,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=10,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/data/pg-macocu/MT_vs_HT/experiments/10/models/google/microsoft-deberta-v3-large_lr=1e-05_bsz=32_seed=1_eval/runs/Mar30_13-52-54_pg-gpu08,
logging_first_step=False,
logging_steps=200,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
no_cuda=False,
num_train_epochs=10,
output_dir=/data/pg-macocu/MT_vs_HT/experiments/10/models/google/microsoft-deberta-v3-large_lr=1e-05_bsz=32_seed=1_eval,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=64,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=microsoft-deberta-v3-large_lr=1e-05_bsz=32_seed=1_eval,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['mlflow', 'tensorboard'],
resume_from_checkpoint=None,
run_name=/data/pg-macocu/MT_vs_HT/experiments/10/models/google/microsoft-deberta-v3-large_lr=1e-05_bsz=32_seed=1_eval,
save_on_each_node=False,
save_steps=200,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=2,
seed=1,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0,
)
  0%|          | 0/43 [00:00<?, ?it/s]  5%|▍         | 2/43 [00:00<00:08,  4.80it/s]  7%|▋         | 3/43 [00:00<00:11,  3.40it/s]  9%|▉         | 4/43 [00:01<00:13,  2.95it/s] 12%|█▏        | 5/43 [00:01<00:13,  2.73it/s] 14%|█▍        | 6/43 [00:02<00:14,  2.61it/s] 16%|█▋        | 7/43 [00:02<00:14,  2.54it/s] 19%|█▊        | 8/43 [00:02<00:14,  2.49it/s] 21%|██        | 9/43 [00:03<00:13,  2.46it/s] 23%|██▎       | 10/43 [00:03<00:13,  2.44it/s] 26%|██▌       | 11/43 [00:04<00:13,  2.43it/s] 28%|██▊       | 12/43 [00:04<00:12,  2.41it/s] 30%|███       | 13/43 [00:05<00:12,  2.41it/s] 33%|███▎      | 14/43 [00:05<00:12,  2.41it/s] 35%|███▍      | 15/43 [00:05<00:11,  2.41it/s] 37%|███▋      | 16/43 [00:06<00:11,  2.40it/s] 40%|███▉      | 17/43 [00:06<00:10,  2.37it/s] 42%|████▏     | 18/43 [00:07<00:10,  2.37it/s] 44%|████▍     | 19/43 [00:07<00:10,  2.37it/s] 47%|████▋     | 20/43 [00:07<00:09,  2.38it/s] 49%|████▉     | 21/43 [00:08<00:09,  2.37it/s] 51%|█████     | 22/43 [00:08<00:08,  2.37it/s] 53%|█████▎    | 23/43 [00:09<00:08,  2.37it/s] 56%|█████▌    | 24/43 [00:09<00:08,  2.36it/s] 58%|█████▊    | 25/43 [00:10<00:07,  2.36it/s] 60%|██████    | 26/43 [00:10<00:07,  2.37it/s] 63%|██████▎   | 27/43 [00:10<00:06,  2.38it/s] 65%|██████▌   | 28/43 [00:11<00:06,  2.38it/s] 67%|██████▋   | 29/43 [00:11<00:05,  2.38it/s] 70%|██████▉   | 30/43 [00:12<00:05,  2.39it/s] 72%|███████▏  | 31/43 [00:12<00:05,  2.39it/s] 74%|███████▍  | 32/43 [00:12<00:04,  2.39it/s] 77%|███████▋  | 33/43 [00:13<00:04,  2.39it/s] 79%|███████▉  | 34/43 [00:13<00:03,  2.39it/s] 81%|████████▏ | 35/43 [00:14<00:03,  2.40it/s] 84%|████████▎ | 36/43 [00:14<00:02,  2.40it/s] 86%|████████▌ | 37/43 [00:15<00:02,  2.40it/s] 88%|████████▊ | 38/43 [00:15<00:02,  2.40it/s] 91%|█████████ | 39/43 [00:15<00:01,  2.40it/s] 93%|█████████▎| 40/43 [00:16<00:01,  2.40it/s] 95%|█████████▌| 41/43 [00:16<00:00,  2.38it/s] 98%|█████████▊| 42/43 [00:17<00:00,  2.39it/s]100%|██████████| 43/43 [00:17<00:00,  2.54it/s]100%|██████████| 43/43 [00:20<00:00,  2.14it/s]

Info:
 {'eval_loss': 0.535869300365448, 'eval_accuracy': 0.9420289855072463, 'eval_f1': 0.9419191919191918, 'eval_precision': 0.9453968253968253, 'eval_recall': 0.9420289855072463, 'eval_runtime': 17.978, 'eval_samples_per_second': 152.297, 'eval_steps_per_second': 2.392}
