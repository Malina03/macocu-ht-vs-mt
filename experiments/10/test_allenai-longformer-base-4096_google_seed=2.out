Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
***** Running Evaluation *****
  Num examples = 290
  Batch size = 64
Initializing global attention on CLS token...
Input ids are automatically padded from 886 to 1024 to be a multiple of `config.attention_window`: 512
=> Loading train corpus...
=> Loading test corpus...
Loading model at /data/pg-macocu/MT_vs_HT/experiments/10/models/google/allenai-longformer-base-4096_lr=1e-05_bsz=32_epochs=15_seed=2/checkpoint-400
Generated by command:
python classifier_trf_hf.py --root_dir /data/pg-macocu/MT_vs_HT/experiments/10 --load_model /data/pg-macocu/MT_vs_HT/experiments/10/models/google/allenai-longformer-base-4096_lr=1e-05_bsz=32_epochs=15_seed=2/checkpoint-400 --arch allenai/longformer-base-4096 --test google --use_google_data
Logging training settings
 TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_steps=200,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
greater_is_better=True,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=10,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/data/pg-macocu/MT_vs_HT/experiments/10/models/google/allenai-longformer-base-4096_lr=1e-05_bsz=32_epochs=10_seed=1_test/runs/Apr28_14-19-23_pg-gpu12,
logging_first_step=False,
logging_steps=200,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1,
max_steps=-1,
metric_for_best_model=accuracy,
mp_parameters=,
no_cuda=False,
num_train_epochs=10,
output_dir=/data/pg-macocu/MT_vs_HT/experiments/10/models/google/allenai-longformer-base-4096_lr=1e-05_bsz=32_epochs=10_seed=1_test,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=64,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=allenai-longformer-base-4096_lr=1e-05_bsz=32_epochs=10_seed=1_test,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['mlflow', 'tensorboard'],
resume_from_checkpoint=None,
run_name=/data/pg-macocu/MT_vs_HT/experiments/10/models/google/allenai-longformer-base-4096_lr=1e-05_bsz=32_epochs=10_seed=1_test,
save_on_each_node=False,
save_steps=200,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=2,
seed=1,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0,
)
  0%|          | 0/5 [00:00<?, ?it/s]Initializing global attention on CLS token...
Input ids are automatically padded from 886 to 1024 to be a multiple of `config.attention_window`: 512
 40%|████      | 2/5 [00:02<00:03,  1.12s/it]Initializing global attention on CLS token...
Input ids are automatically padded from 886 to 1024 to be a multiple of `config.attention_window`: 512
 60%|██████    | 3/5 [00:04<00:03,  1.59s/it]Initializing global attention on CLS token...
Input ids are automatically padded from 886 to 1024 to be a multiple of `config.attention_window`: 512
 80%|████████  | 4/5 [00:06<00:01,  1.84s/it]Initializing global attention on CLS token...
Input ids are automatically padded from 886 to 1024 to be a multiple of `config.attention_window`: 512
100%|██████████| 5/5 [00:07<00:00,  1.62s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
100%|██████████| 5/5 [00:17<00:00,  3.55s/it]

Info:
 {'eval_loss': 1.599755883216858, 'eval_accuracy': 0.6241379310344828, 'eval_f1': 0.5850562490975202, 'eval_precision': 0.6991758241758241, 'eval_recall': 0.6241379310344828, 'eval_runtime': 11.4705, 'eval_samples_per_second': 25.282, 'eval_steps_per_second': 0.436}
